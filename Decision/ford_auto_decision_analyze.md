# 福特自动驾驶决策部分论文分析

## 1. Sub-Game Perfect Nash Equilibrium

## 1.1 理论基础

这里，我们将标准的非合作博弈论解概念应用于确定性动态环境：

Markov sub-game perfect Nash equilibrium(马尔科夫子博弈完美纳什均衡)。每个agent的目标是最大化累计效用函数。 定义如下：
$$
U_i(a_{i,\{t=0,1,...,T-1\}}|s_{i,0},a_{-i,\{t=0,1,...,T-1\}})=\sum^{T-1}_{t=0}u_{i,t}(a_{i,t}|s_{i,t},a_{-i,t}) \tag 1
$$
  从公式中可以看出，需要将其他agent的行为也需要考虑进来，由于我们的规划时间只有几秒钟，因此这里也不需要引入折扣因子。                                                                                          

对于给定其他agents一组固定动作$(a_{-i,\{t = 0,1,...,T-1\}})$后，agent $i$的最佳响应是通过计算一系列自车动作的累计函数后得到的最大值对应的那个动作。
$$
a^*_{i,\{t=0,1,...,T-1\}}(s_{i,0,a_{-i,\{t=0,1,...,T-1\}}}) = \mathop{arg\max}\limits_{a_{i,\{t=0,1,...,T-1\}}} U_i(a_{i,\{t=0,1,...,T-1\}}|s_{i,0},a_{-i,\{t=0,1,...,T-1\}}) \tag 2
$$
注意，上面的方程等效地变成了所有自定义序列的“静态但同步”优化问题$a_{i,t},\forall t \in \{0,1,...,T-1\}$,因此，避开了与动态规划相关的所有计算问题。特别是，维度的诅咒和价值函数的表示并没有出现在我们的语境中。博弈的纳什均衡是当所有主体同时使用各自的最佳对策相互对抗时达到的，即:
$$
a^*_{i,\{t=0,1,...,T-1\}}(s_{i,0,a^*_{-i,\{t=0,1,...,T-1\}}}) = \mathop{arg\max}\limits_{a_{i,\{t=0,1,...,T-1\}}} U_i(a_{i,\{t=0,1,...,T-1\}}|s_{i,0},a^*_{-i,\{t=0,1,...,T-1\}}) \tag 3
$$
这个条件对于验证由某些数值过程得到的解是否确实是纳什均衡特别有用。上述公式的一个微妙之处在于，我们只在优化过程中明确地施加初始条件，而隐式地保留终端条件$s_{i,T}$，本质上作为优化的结果。我们选择以拉格朗日意义上的软方式，通过增大效用函数来强制执行终点条件，例如目的地。这种选择通常会使优化问题变得更容易，但也会产生某些最终结果，例如在物理上不可能到达目的地时避免事故或形成交通堵塞。

我们也可以重写式1的优化问题：
$$
W_{i,t}(s_{i,t}|a_{-i,\{t,...,T-1\}}) = \max_{a_{i,t}}\left\{
u_{i,t}(a_{i,t}|s_{i,t},a_{-i,t}) + W_{i,t+1}(s_{i,t+1}=f_{i,t}(a_{i,t}|s_{i,t},a_{-i,t})|a_{-i,\{t+1,...,T-1\}})
\right\} \tag 4
$$
utility-to-go函数$W_{i,t}()$与状态演变相结合，定义为：
$$
W_{i,t+1}(s_{i,t+1}|a_{-i,\{t+1,...,T-1\}}) = \max_{a_{i,\{t+1,...,T-1\}}} \left\{
\sum^{T-1}_{t'=t+1} u_{i,t'}(a_i,t'|s_{i,t'},a_{-i,t'})
\right\} \tag 5
$$
公式4是动态规划中的贝尔曼方程，同时也表明了公式2中的纳什均衡也是子博弈完美的均衡。

## 1.2 Algorithm 1:betaNash

利用最佳响应动力学方法，对纯策略下的马尔可夫子博弈完美纳什均衡进行了数值求解。基本思想是，从一些合理的初始行动序列开始，每个智能体在给定所有其他智能体的主流策略的情况下，试图在每次迭代中做出最优响应(视为自玩(self-play)学习过程)。在一定的数学条件下，最佳响应动力学收敛于纳什均衡。这类博弈的一个广泛类别，称为纯策略的超模博弈。正式地，最佳响应动力学定义为：
$$
a^*_{i,\{t=0,1,...,T-1\},\tau+1}(s_{i,0},a_{-i,\{t=0,1,...,T-1\},\tau}) =
\mathop{\arg\max}_{a_{i,\{t=0,1,...,T-1\}}}U_i(a_{i,\{t=0,1,...,T-1\}}|s_{i,0},a_{-i,\{t=0,1,...,T-1\},\tau}) \tag 6
$$
其中我们添加了下标$\tau$，以表示最佳响应动力学的迭代步长。值得指出的是，虽然公式中没有显式的协调，但实际的协调是由上述方程的最佳响应动力学的迭代性实现的。某种意义上，最佳响应迭代提供了“协商过程”，用于所有agent来理解其他agent的意图，例如，以什么顺序以什么速度去哪里，以便所有相关的agent都能获得一个相互满意的结果。

我们还认识到，最佳反应动力学是一种特定形式的自我游戏强化学习，最初是为了证明纳什均衡的存在，纳什均衡在数学上如此复杂，并且只能被具有一般有限理性的游戏玩家找到。虽然详细的状态表示、函数逼近和搜索策略完全不同，至少在精神上，它与DeepMind的alphaGo Zero使用的自对弈强化学习非常相似。在我们的环境中不需要神经网络表示，因为我们知道效用函数和状态演化的显式函数形式。在最佳响应动力学中，传统的最大化方法比基于随机梯度或树搜索的方法计算效率更高。

有了以上的数学准备，我们将算法表述如下:

![ford_figure1](E:\PnC\pnc_research\elements\ford_figure1.PNG)

翻译一下：

1. 初始化模拟环境

2. 对$\forall i \in I$   **do**

   1. 初始化状态变量 $s_{i,t}=0$
   2. 初始化动作集合 $a_{i,\{t=0,1,...,T-1\},\tau = 0}=0$

3. **for**  *iteration step*$\tau = 1 : n$   **do**  //这里的循环用于寻找最佳响应

   1. for agent i = 1:N do

      1. 获取上一步最佳动作序列：$a_{-i,\{t=0,1,...,T-1\},\tau-1}$

      2. 对当前agent进行优化：

         1. $$
            a^*_{i,\{t=0,1,...,T-1\}}(s_{i,0,a_{-i,\{t=0,1,...,T-1\}}}) = \mathop{arg\max}\limits_{a_{i,\{t=0,1,...,T-1\}}} U_i(a_{i,\{t=0,1,...,T-1\}}|s_{i,0},a_{-i,\{t=0,1,...,T-1\}})
            $$

      3. 更新当前agent的动作集：$a_{i,\{t=0,i,...,T-1\},\tau} \leftarrow a^*_{i,\{t=0,1,...,T-1\}}$

   2. 检查纳什均衡条件：
      $$
      a^*_{i,\{t=0,1,...,T-1\}}(s_{i,0,a^*_{-i,\{t=0,1,...,T-1\}}}) = \mathop{arg\max}\limits_{a_{i,\{t=0,1,...,T-1\}}} U_i(a_{i,\{t=0,1,...,T-1\}}|s_{i,0},a^*_{-i,\{t=0,1,...,T-1\}})
      $$

   3. 如果条件为真，则跳出迭代，否则继续。

4. **for $\forall i \in I$  do**

   1. **for $t = 0,1,...,T-1$ do**

      1. $s_{i,t+1} = f_{i,t}(a^*_{i,t}|s_{i,t},a^*_{-i,t})$

      **end**

     **end**

该算法有如下几个好的性质：

- 它基于一个严格的马尔可夫博弈，其纳什均衡可以通过称为最佳响应动力学的系统数值方法来求解。
- 所得到的动态纳什均衡通过构造自动实现子博弈完善
- 最佳反应动力学提供了一种明确的协商或学习机制，用于确定所有代理的驱动意图和最优行为
- 虽然博弈论的假设非常强大，因此衍生的子博弈完美纳什均衡解决方案可能与人类驾驶员参与的混合设置建模不立即相关，但它们至少可以作为理论基准(banchmark)。我们直觉地认为，基于博弈论的解决方案往往比启发式动机的解决方案更有效
- 同样可以想象的是，在即将到来的互联和自动驾驶世界中，如果附近所有车辆的决策都可以集中，那么可以通过边缘基础设施系统实施相同的解决方案。
- 该算法在计算上是可行的，至少对于一些小而现实的问题，其中游戏中的代理数量不是太大。

缺点如下：

- 与动态纳什均衡相关的假设可能太强而不够现实。当然，这也是对博弈论基本假设相关性的批评，例如常识(例如，知道所有代理的效用函数)和完全合理性(例如，无限计算能力)。
- 也许只有当所有的智能体都是自治的，它们的效用函数是被赋予的，并且解决方案是由一个中央控制器导出时，共同知识假设才能实现
- 当代理数量变大时，计算负担会急剧增加，至少为$N^2\times T$​，最佳响应动力学中的迭代次数，其中N是博弈中涉及的代理的数量。这使得它很难应用于N较大的现实环境中。
- 正如公式(6)所定义的那样，最佳响应动态本质上是顺序的，因此betaNash不是自然并行的。通过在一个时间步之前同时计算对所有其他代理的动作的最佳响应，可以使算法并行，尽管这可能会在每次迭代的基础上损失一些效率
- 另一个问题是，在整个计划时间窗口内，代理的数量N必须固定。考虑到规划时间可能长达10秒，这是不太现实的。
- 常识和合理性假设，加上沉重的计算负担，也会限制在逆问题和机构设计问题中的应用范围。

正是为了克服这些弱点，促使我们追求另一个解决方案概念，详见下一节。

## 2 Adaptive Optimization with Finite Look-Ahead Anticipation

在上一节中，我们在效用函数中引入了一个明确的风险溢价，这样，至少从状态演化的角度来看，可能的随机博弈可以简化为一个完全信息的博弈。因此，大大减轻了计算负担。然而，我们有更多的理由进一步简化模型，以便我们能够同时适应人类驾驶员和自动驾驶汽车。这是由于几点考虑。首先，一旦处于人类主体和算法主体共存的混合环境中，常识假设就会变得可疑。当然，我们总是可以撤销贝叶斯纳什均衡概念来放宽一些常识要求。但这将再次证实计算负担。其次，假设具有有限理性的人类代理人总能在现实交通环境中实时找到相关的纳什均衡是不合理的。第三，为了使该方法切实可行，需要进一步减少计算量。

在我们更正式地继续之前，让我们首先回顾一下人类驾驶启发法：

- 所有的决定都是分散的，由每个代理单独做出，大部分没有明确的沟通。
- 所有的即时行动都是根据当前状态来计划的，并带有一些合理的或足够好的但对未来状态的不精确的预期。
- 如果未来状态有多种情况，则根据最保守的情况进行行动计划。
- 计划的行动具有一定的时间持久性，除非它们被环境变化打断。
- 一旦评估了新的信息，行动计划就会重新开始。

### 2.1 理论基础

有了第1节中的公式，我们就有了一个很好的起点，从这个起点我们可以逐渐放宽纳什均衡方法最初所需的条件。继续使用人类驾驶员的启发知识来进行分析，重点是从前一节的完整博弈和相关均衡的角度转移到本节的个体主体决策的角度。

第一步是简化方程(2)$s_{i,t+1}=f_{i,t}(a_{i,t}|s_{i,t},a_{-i,t})$中的状态演化。回想一下，我们使用状态演化来消除累积效用函数对未来状态的所有依赖。虽然这在理论上是很自然的，但很难想象人类司机可以在现实的交通环境中多次迭代状态进化，因为在这种情况下，自己和他人的未来动作序列都是需要的。更有可能的是，人类驾驶员将通过推断当前状态、自己的直接行动以及对随后所有其他代理的行动序列的一些合理假设，简单地预测未来的状态。这激发了以下估计的状态演变：
$$
\tilde{s}_{i,t+1}=\tilde{f}_{i,t}(a_{i,t}|s_{i,t}) \tag 7
$$
在上述方程中，我们通过假设$a_{-i,t}$是一些与特定环境相称的自然直觉操作，这样做保持了自我行为依赖，并放弃了对所有其他智能体行为的精确依赖。然而。公式7不能对$a_{-i,t}$完全独立，否则我们会处理不相互作用的情况。因此，我们将对其他智能体的依赖放入了环境中$s_{i,t}$，我们的策略是将其他agent的精确地动态交互的动作序列替换为与上下文合理一致的一些规定的动作序列。在本小节的其余部分中，我们概述了有限前视预期所必需的两个一般元素。当然，预期的的详细形式是高度依赖于上下文的，因此不能通用。我们将在第3节的模拟实验中提供一个具体的示例，包括算法细节。

下一步是简化公式(4)中的最佳响应。一个显而易见的方法是将规划范围分成更小的部分,极端的case就是T=1,不幸的是，我们从经验上知道，纯粹的短视决策是不够的。因此，需要在过程中建立某种形式的预期,为此，我们通过前视 h >> 1 个周期来定义有效的效用函数：
$$
\tilde u_{i,t}(a_{i,t}|s_{i,t};h) = \sum_k w_{i,k}g_k(\phi^{(k)}_{i,t}(a_{i,t}|\tilde s_{i,t});h) \tag 8
$$
$g_k()$函数形式的选择取决于具体的特性，出于安全考虑，在必要时采用保守形式。对于一些组件，如前进奖励和车道偏离惩罚,$g_k()$为对应分量在h周期内的平均值。为了驾驶的平稳性，我们选择在第一时刻就进行惩罚。对于具有潜在灾难性的组件，例如碰撞或碰撞惩罚，我们在给定其他操作序列的h周期中选择最大惩罚。一旦所有路径都被假设，方程（8）中定义的有效效用函数就体现了我们所说的有限前瞻预期的第一个元素。

然后，得到的近似最佳动作可由：
$$
\tilde a^*_{i,t}(s_{i,t};h) = \mathop{\arg\max}_{a_{i,t}}\tilde{u}_{i,t}(a_{i,t}|s_{i,t};h) \tag9
$$
注意，由于在简化的有效效用函数中删除了所有其他代理的行动序列，因此上述优化只是对预期中预先选择的行动序列的反应，因此原始博弈问题被逐步简化为控制问题。

当Eq.(9)中没有明确的协调机制时，一个微妙的问题就出现了，在每个agent没有共同知识的情况下，每个agent如何弄清楚附近所有其他agent的意图?为了解决这个问题，我们引入了路径场景的概念:附近的智能体在任何给定时刻可以追求的有限数量的可能路径。这可以通过扩大式(9)中的状态空间来实现。同样，我们采用一种保守的方法，以最保守的路径方案来规划智能体i的行动。这样，状态变量也应该被理解为自适应的，包括附近代理的数量和它们的路径场景。这可以看作是有限前瞻预期的第二个要素。重要的是要强调，对未来状态的预期是针对所有agent的，包括自我和所有其他agent，尽管它们被不同地对待，因为自我意图是已知的，而他人意图是未知的。

从式(9)中解出$\tilde a^*_{i,t}(s_{i,t};h)$只执行一个周期，尽管它是由H个周期的信息派生出来的。随着时间从t到t + 1的增加，从t + 1处的可观测状态变量重新评估一个新的优化问题，这是我们制定建模框架的第三步，该框架最终能够处理人类驾驶行为，同时在计算上可行。这种建模策略背后的直觉是，每个智能体在微观层面上有计划地规划自己的路径，然后不断地监控情况，并在必要时进行调整。当然，这种建模策略是否能真正很好地近似人类驾驶行为，是否足以避免事故等问题，只能凭经验来回答。

最后如何对h进行选取，首先不应该过大，这是因为当h非常大时，$a_{-i,t}$的假设变得非常不可靠，决策结果也会被影响。一般来说，h应被视为模型的超参数，并在每个特定的建模情况下进行调优。然而，我们的经验表明，h ×∆t在1到3秒的范围内是一个很好的选择，根据具体的交通状况，如本地或高速公路，拥挤或不拥挤，好天气或坏天气等，为许多不同的驾驶设置提供鲁棒性。

### 2.2 Algorithm Ⅱ：adaptiveSeek

![image-20240123212524917](C:\Users\rck\AppData\Roaming\Typora\typora-user-images\image-20240123212524917.png)

翻译一下：

- **for** $forall i \in I$ **do**

  - 初始化仿真环境

  **end for**

- **for** $t=0:T-1$  **do**

  - **for** $\forall i \in I$ **do**   //这里对所有的agent进行操作

    - 获取当前的状态$s_{i,t}$

    - **for** $\forall j \in I_{-i}$ //对于除i之外其他的agents

      - 如果评估某些期望的路径是可行的

        - 根据智能体j的状态，预测其可能采取的路径方案

        - **for** $\tilde t=0:h-1$ **do**

          - 使用与路径场景一致的规定动作来推断轨迹

          **end for**

      - **else**

        - 假设智能体j保持其当前状态的自然扩展

        - **for** $\tilde t=0:h-1$ **do**

          - 不采取额外动作去推断轨迹

          **end for**

      - **end if**

    - **end for**

  - 计算给定所有预期路径情景的有效效用:
    $$
    \tilde u_{i,t}(a_{i,t}|s_{i,t};h) = \sum_k w_{i,k}g_k(\phi^{(k)}_{i,t}(a_{i,t}|\tilde s_{i,t});h)
    $$

  - 在所有预期的路径场景中选择具有最安全那条路径

  - 做优化：
    $$
    \tilde a^*_{i,t}(s_{i,t};h) = \mathop{\arg\max}_{a_{i,t}}\tilde{u}_{i,t}(a_{i,t}|s_{i,t};h)
    $$

  - **If**需要添加噪音
    $$
    \hat a_{i,t}(s_{i,t})=\tilde a^*_{i,t}(s_{i,t};h)+\epsilon_{i,t}
    $$
    **end if**

  - **end for**

  - **for $\forall i \in I$  do**

    - 应用观察到的动作并演化到新的状态$s_{i,t+1} = f_{i,t}(\hat a_{i,t}|s_{i,t})$

    **end for**

- **end for**

<font color=red>Note:个人觉得这里复杂度仍然是很高，它对每个agent进行操作时，都要对其他agent进行评估，得到其未来轨迹的一系列状态，作为求当前agent当前时刻最优动作</font>

以下是对上述算法(adaptiveseek)的一些一般性评论:

- 由于在adaptiveSeek中，每个agent的决策都是独立的，没有明确的协调，所以不需要假设博弈中的所有agent都遵循相同的算法。可能有些agent是人类驱动的，有些agent是自主的，有自己的内置算法，只要所有隐含的行为都可以合理地预测。因此，我们期望adaptiveSeek可以用于建模所有类型的代理共存的混合设置。
- 由于缺乏共同知识，我们不能假设每个代理都知道所有其他代理的意图。因此，这需要对附近agent的意图进行某种形式的预测。我们选择用最简单的方法来实现这一点——使用与路径场景一致的规定动作序列。然后，算法在所有可能的场景中选择最安全的最优操作来执行。通过这种方式，我们能够处理混合环境。优化过程中的if-else条件是每个智能体不知道其他智能体意图的直接结果。
- 然而，由于在有效效用函数中包含了对所有附近代理的预期，adaptiveSeek仍然是隐式协调的，而betaNash的协调是显式的。如果没有某种形式的协调，就不可能照顾到所有代理之间的相互作用。
- 通过这种完全自适应的方法，我们可以轻松地处理在任何给定时间进入或离开模拟环境的任何代理，这使得它比算法betaNash中的解决方案概念更灵活。
- 通过构造，该算法是局部的，因为效用函数只依赖于附近的agent，因此在参与博弈的agent数量上是线性的。此外，内环可以很容易地并行化，因为每个智能体在每个时间步长的决策实际上是相互独立的。如果处理器的数量足够大，这个算法实际上是o (n)。或者，这种内部循环优化可以分配到每个代理自己的车载计算机上。正是这些好的属性使得adaptiveSeek非常快。这反过来又为它在实际应用中实现实时提供了一个很好的机会。

### 2.3 加入不确定性

加入不确定性的原因为：

1. 检测模型对小扰动的鲁棒性
2. 利用最大似然或最大熵方法开发的逆强化学习技术解决逆问题
3. 为提高交通规则设计良好的机制

为此，我们需要区分决策者感知到的状态和实际实现的状态。最终为：
$$
\begin{cases}
\hat s_{i,t} = s_{i,t} + \epsilon^s_{i,t}\\
a^*_{i,t}(\hat s_{i,t};h) = \mathop{\arg\max}_{a_{i,t}}\tilde u_{i,t}(a_{i,t},\hat s_{i,t};h) \\
\hat a_{i,t}(\hat s_{i,t}) = a^*_{i,t}(\hat s_{i,t};h) + \epsilon^a_{i,t} \\
s_{i,t+1}=\tilde f_{i,t}(\hat a_{i,t}(\hat s_{i,t})|s_{i,t})
\end{cases} 
\tag{10}
$$


$\epsilon_{i,t}$为一些统计学上的分布。虽然为了简化符号，我们在上述方程中添加了噪声项，但将其中一些项相乘处理可能更合适。请注意，实现的行动是由两个不确定性来源造成的，一个是显式的$\epsilon^a_{i,t}$,另一个是隐式的$\epsilon^s_{i,t}$，由于状态演化应该是内在的，而不是从决策的角度出发，所以实际状态是从实际初始状态演化通过实际动作$\hat a_{i,t}(\hat s_{i,t})$而来的。Eq.(10)中噪声项的来源可能是由于仪器噪声、人为估计和理想状态和最佳动作的执行错误，也可能是由于道路的小不确定性，如局部斜坡、颠簸和坑洞、恶劣的天气和照明条件。具体的分布假设取决于建模的上下文。它们可以像IID正态分布一样简单，也可以更复杂，比如包含某种形式的自回归结构。

在某种意义上，上述规范在后验上调和了我们早期的确定性公式与博弈的常见MDP公式，其中状态进化是随机的，行动可以采取混合策略。类似地，尽管有额外的符号复杂性，我们也可以在第1节中重新编写算法betaNash的状态演化和动作优化。在这种情况下，子博弈的完全纳什均衡条件需要理解为上述确定性等价近似下的均衡条件。即便如此，平衡也必须一段一段地解决，不仅要适应现在被噪音或错误污染的实现状态，还要适应在任何给定时刻参与游戏的代理的潜在变化。

### 2.4 Specification of the Utility Function

效用函数有八个组成部分，它们的解释和函数形式如下所述:

- 一种前进的奖励，在高速公路的限速处达到顶峰 $v_0 = 31 m/s$

$$
\phi^{(1)}_{i,t}=1-\left(\frac{v_{i,t}-v_0}{v_0}\right)^2
$$

- 三个不平整的处罚，两个鼓励加速/转向平滑随着时间的推移，和一个阻止硬加速和制动。

$$
\phi^{(2)}_{i,t}=(\alpha_{i,t} - \alpha_{i,t-1})^2 \\
\phi^{(3)}_{i,t}=(\delta_{i,t} - \delta_{i,t-1})^2 \\
\phi^{(4)}_{i,t} = ln\left(1 + \exp\left[ \kappa^{(4)}(\alpha^{i,t} - \overline \alpha)\right]\right) + ln\left(1 + \exp\left[ -\kappa^{(4)}(\alpha^{i,t} - \underline\alpha)\right]\right)
$$

​		其中$\overline \alpha = 4m/s$且$\underline \alpha = -5m/s$为加速度和减速度的边界。参考$\kappa^{(4)} = 15.0$表示表示车辆在硬加速和硬制动		时的物理约束的硬度。

- 一种车道偏离惩罚，激励车辆保持在以±1:85米为中心的车道中间

$$
\phi^{(5)}_{i,t} = \min\left[
\left(y^2_{i,t} - (W/2)^2\right)^2 / (3W^4/4) ,1
\right]
$$

​		其中W= 3:7 m为车道宽度，两条车道以±W=2

- 当车辆离开路肩时，在道路外的惩罚会给你很大的惩罚。
  $$
  \phi^{(6)}_{i,t} = S\left(
  \kappa^{(6)}\left(
  |y_{i,t} - (W + w/2)
  \right)
  \right)
  $$
  其中w=2，表示车宽，$S(x)=1/[1+exp(-x)]$ 为sigmoid函数，且参数$\kappa^{(6)} = 3.0$用于控制惩罚项的速度

- 在意外障碍物x = 0处的碰撞惩罚。
  $$
  \phi^{(7)}_{i,t} = S\left(\kappa^{(7)}(x_{i,t}+l^{(7)}_x\right) \cdot 
   S\left(-\kappa^{(7)}(y_{i,t}-l^{(7)}_y\right)
  $$
  撞击风险参数为$l^{(7)}_x = 5.0m$，且$l^{(7)}_y=1.0m$，参数$\kappa^{(7)}_x=2.0，\kappa^{(7)}_y=20.0$控制风险溢价在纵向和横向上降至零的速度，

- 假设相邻车辆(成对)的形状为矩形的碰撞惩罚。

$$
\phi^{(8)}_{ij,t} = 
\left[
\tilde S\left(\kappa^{(8)}_x(\Delta x_{ij,t} + l^{(8)}_x)\right) +
\tilde S\left(\kappa^{(8)}_x(l^{(8)}_x -\Delta x_{ij,t})\right)
\right]\cdot \\
\left[
\tilde S\left(\kappa^{(8)}_y(\Delta y_{ij,t} + l^{(8)}_y)\right) +
\tilde S\left(\kappa^{(8)}_y(l^{(8)}_y -\Delta y_{ij,t})\right)
\right]
$$

其中$\Delta x_{ij,t} = x_{i,t} - x_{j,t}$,$\Delta y_{ij,t} = y_{i,t} - y_{j,t}$，且$\tilde S(x) = S(x) - 1/2$。另外$l^{(8)}_x = 10.0m,l^{(8)}_y = 2.0m,\kappa^{(8)}_x = 0.5,\kappa^{(8)}_y = 2.0$

相应的权重设置为$w_1 = 1.0,w_2 = -0.01,w_3 = -1.5,w_4 = -1.0,w_5 = -0.3,w6 = -24.0,w_7 = -20.0,w_8 = -14.0$,

请注意，前六个组成部分是纯粹的物理自我效应，只有后两个涉及相互作用和风险溢价。